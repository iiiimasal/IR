# -*- coding: utf-8 -*-
"""preprocessing-context.ipyn

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oTAw7qIvTkc54QHBOHH_7VtmiiJ9kc4b
"""

import nltk
print(nltk.__version__)

import nltk
nltk.download('stopwords', download_dir='/usr/local/nltk_data')
import nltk
nltk.download('wordnet', download_dir='/usr/local/nltk_data')

!pip install wikipedia-api

import nltk
import string
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
import ssl

def preprocess_text(content):

    try:
        _create_unverified_https_context = ssl._create_unverified_context
    except AttributeError:
        pass
    else:
        ssl._create_default_https_context = _create_unverified_https_context

    # Ensure necessary NLTK data is downloaded
    nltk.download('stopwords', quiet=True)
    nltk.download('punkt', quiet=True)
    nltk.download('wordnet', quiet=True)

    # Tokenization
    tokens = word_tokenize(content)

    # Remove punctuation
    tokens = [word for word in tokens if word not in string.punctuation]

    # Lowercasing
    tokens = [word.lower() for word in tokens]

    # Removing stopwords
    stop_words = set(stopwords.words('english'))
    tokens_no_stopwords = [word for word in tokens if word not in stop_words]

    # Lemmatization
    lemmatizer = WordNetLemmatizer()
    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens_no_stopwords]

    return lemmatized_tokens

import requests
import wikipediaapi
import re
from urllib.parse import unquote  # To decode the URL

# Function to extract Wikipedia page title from URL
def extract_title_from_url(url):
    # Decode the URL to handle special characters like '%27' (apostrophe)
    decoded_url = unquote(url)
    # Extract title from a Wikipedia URL
    match = re.search(r"\/wiki\/(.+)$", decoded_url)
    if match:
        return match.group(1)
    else:
        return None

# Function to fetch Wikipedia content based on the page title
def get_wikipedia_content(page_title, lang="en"):
    user_agent = "MyWikipediaBot/1.0 (Contact: your_email@example.com)"
    wiki_wiki = wikipediaapi.Wikipedia(language=lang, user_agent=user_agent)

    page = wiki_wiki.page(page_title)
    if not page.exists():
        print(f"Page '{page_title}' not found!")
        return None

    return page.text  # Extract full text content

# URL of the raw text file on GitHub (which contains URLs of Wikipedia pages)
f = "https://raw.githubusercontent.com/iiiimasal/IR/master/source_file"

# Fetch the file containing URLs
response = requests.get(f)
if response.status_code == 200:
    urls = response.text.splitlines()  # Convert text into a list of URLs
    print(urls)
    # Loop through URLs and get content from Wikipedia
    for url in urls:
        print(f"Processing URL: {url}")
        title = extract_title_from_url(url)  # Extract the title from the URL
        if title:
            content = get_wikipedia_content(title)  # Fetch Wikipedia content
            if content:
                print(f"Content from {url}:")
                print(preprocess_text(content))

                print("="*100)  # Separator for readability
        else:
            print(f"Invalid Wikipedia URL: {url}")
else:
    print("Failed to fetch file:", response.status_code)

def preprocess_persian_text(text):
    # 1. Normalize text
    text = normalizer.normalize(text)

    # 2. Tokenize
    tokens = word_tokenize(text)

    # 3. Remove stopwords
    tokens = [word for word in tokens if word not in stopwords and word not in punctuation]
    # 4. Lemmatization (reduce words to their base form)
    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]

    return lemmatized_tokens  # Return processed tokens

from hazm import Normalizer, word_tokenize, Lemmatizer, stopwords_list
import string
# Initialize hazm tools
normalizer = Normalizer()
lemmatizer = Lemmatizer()
stopwords = set(stopwords_list())  # Load Persian stopwords
print(stopwords)
punctuation = set(string.punctuation)

import wikipediaapi
import requests

# Initialize Wikipedia API for Persian
user_agent = "your-application-name/1.0 (your-email@example.com)"
wiki = wikipediaapi.Wikipedia(language='fa', user_agent=user_agent)

# Function to fetch Wikipedia content
def get_wikipedia_persian_content(title):
    formatted_title = title.strip().replace(" ", "_")  # Remove spaces and replace with underscores
    page = wiki.page(formatted_title)

    if page.exists():
        return page.text  # Return full content
    else:
        print(f"Page not found for title: {title} (Formatted: {formatted_title})")
        return None

# URL of the raw text file on GitHub (which contains Wikipedia titles)
url = "https://raw.githubusercontent.com/iiiimasal/IR/master/source_file_persian"

# Fetch the file containing Wikipedia titles
response = requests.get(url)
if response.status_code == 200:
    titles = response.text.splitlines()  # Convert text into a list of titles
    print("Fetched Titles:", [repr(title) for title in titles])  # Show raw format for debugging

    # Loop through titles and fetch content from Wikipedia
    for title in titles:
        title = title.strip()  # Remove extra spaces
        if title:
            content = get_wikipedia_persian_content(title)
            if content:
                print(f"Content from {title}:")
                #print(content[:500])  # Print only the first 500 characters for preview
                print(preprocess_persian_text(content))

                print("="*100)
            else:
                print(f"Failed to fetch content from {title}")
        else:
            print("Invalid Wikipedia title (empty)")
else:
    print("Failed to fetch file:", response.status_code)

!pip install hazm