# -*- coding: utf-8 -*-
"""preprocessing-context.ipyn

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oTAw7qIvTkc54QHBOHH_7VtmiiJ9kc4b
"""

import nltk
print(nltk.__version__)

import nltk
nltk.download('stopwords', download_dir='/usr/local/nltk_data')
import nltk
nltk.download('wordnet', download_dir='/usr/local/nltk_data')

import nltk
import string
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
import ssl

def preprocess_text(content):

    try:
        _create_unverified_https_context = ssl._create_unverified_context
    except AttributeError:
        pass
    else:
        ssl._create_default_https_context = _create_unverified_https_context

    # Ensure necessary NLTK data is downloaded
    nltk.download('stopwords', quiet=True)
    nltk.download('punkt', quiet=True)
    nltk.download('wordnet', quiet=True)

    # Tokenization
    tokens = word_tokenize(content)

    # Remove punctuation
    tokens = [word for word in tokens if word not in string.punctuation]

    # Lowercasing
    tokens = [word.lower() for word in tokens]

    # Removing stopwords
    stop_words = set(stopwords.words('english'))
    tokens_no_stopwords = [word for word in tokens if word not in stop_words]

    # Lemmatization
    lemmatizer = WordNetLemmatizer()
    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens_no_stopwords]

    return lemmatized_tokens

import requests
import wikipediaapi
import re
from urllib.parse import unquote  # To decode the URL

# Function to extract Wikipedia page title from URL
def extract_title_from_url(url):
    # Decode the URL to handle special characters like '%27' (apostrophe)
    decoded_url = unquote(url)
    # Extract title from a Wikipedia URL
    match = re.search(r"\/wiki\/(.+)$", decoded_url)
    if match:
        return match.group(1)
    else:
        return None

# Function to fetch Wikipedia content based on the page title
def get_wikipedia_content(page_title, lang="en"):
    user_agent = "MyWikipediaBot/1.0 (Contact: your_email@example.com)"
    wiki_wiki = wikipediaapi.Wikipedia(language=lang, user_agent=user_agent)

    page = wiki_wiki.page(page_title)
    if not page.exists():
        print(f"Page '{page_title}' not found!")
        return None

    return page.text  # Extract full text content

# URL of the raw text file on GitHub (which contains URLs of Wikipedia pages)
f = "https://raw.githubusercontent.com/iiiimasal/IR/master/source_file"

# Fetch the file containing URLs
response = requests.get(f)
if response.status_code == 200:
    urls = response.text.splitlines()  # Convert text into a list of URLs
    print(urls)
    # Loop through URLs and get content from Wikipedia
    for url in urls:
        print(f"Processing URL: {url}")
        title = extract_title_from_url(url)  # Extract the title from the URL
        if title:
            content = get_wikipedia_content(title)  # Fetch Wikipedia content
            if content:
                print(f"Content from {url}:")
                print(preprocess_text(content))

                print("="*100)  # Separator for readability
        else:
            print(f"Invalid Wikipedia URL: {url}")
else:
    print("Failed to fetch file:", response.status_code)